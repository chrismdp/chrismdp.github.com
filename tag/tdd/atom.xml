---
layout: nil
---
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Chris Parsons: posts tagged tdd</title>

  <link href="http://chrismdp.com/"/>
  <updated>2012-09-20T15:36:32+01:00</updated>
  <id>http://chrismdp.com/tag/tdd</id>
  <author>
    <name>Chris Parsons</name>
    <email>chrismdp@gmail.com</email>
  </author>
<entry><title>Never leave a failing test</title><category term='tdd'/><category term='craftsmanship'/><category term='code'/><category term='testing'/><link href='http://chrismdp.com/2012/09/failing-tests-rot'/><updated>2012-09-20T15:36:32+01:00</updated><id>http://chrismdp.com/2012/09/failing-tests-rot</id><content type='html'>&lt;p&gt;Imagine this: you're taking a guided tour of a nuclear power station. Just above the door as you come in there there are five lights marked Key Safety Indicators. One of the lights is flashing red.&lt;/p&gt;

&quot;What's that flashing red light?&quot; you nervously ask your host.

&quot;Oh, that light does that from time to time. We're not sure why; we just ignore it.&quot;

There's an awkward silence. How confident are you feeling right now?

## Failing tests fester.

Red tests are like code rot. Catch it early and sort them out, and you'll be fine. If you don't, they'll spread through your code like a disease, causing all sorts of damage:

* *Failures cause fear of change.* If we don't understand why a test is failing, we don't understand the code base. If we don't understand our code, we can't change it safely. All bets are off: any change we make will cause us to be that little bit more anxious.

* *Failures breed failures.* If one test continually fails, then other coders are more likely to tolerate failing tests, and the number of failing tests will grow quickly.

* *Failures kill urgency.* There's a scene in a well-known heist movie where a team of thieves has to break into a bank. Their strategy revolves around putting a remote-controlled car under a waste bin: they use this to cause the bin to move at night, setting off all the alarm sensors. The first time the alarm goes off, the place is filled with police in a matter of seconds. The fifth time the alarm goes off, only one squad car with two bored officers turn up, totally unprepared for the waiting thieves who quickly overpower them. The same is true with tests: if they fail all the time, developers will take a cavalier attitude to checking out the cause. This could cause a really serious failure to be missed.

The only point at which failing tests are valid is when you've written them just before the code you plan to add. If the test should be failing, write code to make it work. If the test shouldn't be failing, change it or delete it. Never leave it to fester.
</content></entry><entry><title>The power of feedback</title><category term='lean startup'/><category term='tdd'/><category term='bdd'/><category term='feedback'/><category term='lean'/><link href='http://chrismdp.com/2012/09/the-power-of-feedback'/><updated>2012-09-13T13:52:16+01:00</updated><id>http://chrismdp.com/2012/09/the-power-of-feedback</id><content type='html'>&lt;p&gt;&lt;i&gt;&quot;Everyone has a story that makes me stronger.&quot; -- Richard Simmons&lt;/i&gt;&lt;/p&gt;

There's something about feedback. Whether it's the validation of your latest idea, a hit on your webpage showing up on Google Analytics, or something as simple as a passing test, it's a valuable and important motivational commodity, which can also shape the direction in which we're going very precisely.

The effect of feedback is the engine at the root of software techniques as diverse as pairing, [TDD](http://en.wikipedia.org/wiki/Test-driven_development), [BDD](http://en.wikipedia.org/wiki/Behavior-driven_development) and the [Lean Startup movement](http://theleanstartup.com). Why is feedback so powerful?

## Feedback shortens the loop

Any sort of feedback represents the end of a creative loop that started when we began to work on whatever we're receiving feedback about. The shorter that loop, the more quickly we can respond to change, and the more agile we can be. It also helps us know when we're done working on something and it's time to move on.

That's partly why TDD is so powerful: we receive instant feedback on what we're working on and we are never more than a few minutes away from a fully working system. It's also why good quality customer feedback is powerful: we're never more than a few iterations away from the feature the customer wants.

## Feedback validates us and our work

The validation of our work is one of the things that lies at the root of [pairing](/2010/01/pairing-works-for-everthing): the constant code review and the camaderie keeps us motivated and working on something longer than we can manage on our own. I've found programming on [Sol Trader](http://soltrader.net) alone to be an enlightening experience - I've learnt how important it is to have others working alongside me. I now have a graphics expert reviewing my code, and more design and artistic help to keep me motivated to turn out releases.

It's also incredibly motivating to receive a &quot;thank you!&quot; or &quot;looks great!&quot; There's a lot of power in simple encouragement. If we know our work is appreciated and valued, we'll likely to work longer and with more energy on that next killer feature.

However, there's a danger in only seeking pure validation, or (worse) coming to rely on it for motivatioW. If we receive too much positive validation, we'll end up getting proud of ourselves and demotivated to push for excellence, and we'll get terminally discouraged if we get too little. We should be seeking the kind of feedback that motivates us to shape our work for the better. We have to learn to ask the right questions.

## Feedback shapes our work

If we let it, feedback will change the work we do and how we do it. This applies no matter how we receive feedback about our work - the different types of feedback will change our work in different ways, and we must therefore strive to increase both the quality and the variety of the feedback we receive, without falling into the trap of simple validation.

Done right, TDD offers more that just validation of our code; it gives us information about the quality of our code design. It causes us to shape our code differently and more carefully than code written without feedback. We can't operate in isolation though: TDD without feedback from stakeholders (whether that's through a technique such as Behaviour Driven Development or some other method) is incomplete: we get feedback that our code works, but nothing on whether it's the right code.

There's more: conversations such as [Lean Startup](http://theleanstartup.com/) are taking the BDD ideas one stage further. Instead of relying on the guesses of the stakeholders to determine what the right features are, how about harnessing feedback from the actual customers using the product? This can be done in various ways, through automatic metrics gathering and [tracking experiments rather than features](https://speakerdeck.com/u/chrismdp/p/lean-startup-validated-learning-and-kanban-for-hypothesis).

It's my opinion that the Lean Startup conversation is certainly as important as the BDD conversation, and potentially as important as the Agile conversation, as it improves the variety of the feedback we receive on our work.

How are you finding feedback shapes your work? Are you getting the right kinds of feedback from a variety of sources? Or are you settling for pure validation?
</content></entry><entry><title>How I'm testing iPhone apps: part 2</title><category term='code'/><category term='ios'/><category term='tdd'/><category term='bdd'/><category term='testing ios'/><link href='http://chrismdp.com/2011/12/how-im-testing-iphone-apps-part-2'/><updated>2011-12-06T16:02:54+00:00</updated><id>http://chrismdp.com/2011/12/how-im-testing-iphone-apps-part-2</id><content type='html'>&lt;p&gt;&lt;i&gt;I've recently been doing some iOS development, and working out the best way to test-drive the development of iOS apps was high on my priority list. I know that the automated testing of iOS applications is still not widely practiced and isn't well documented, so I decided to write a series of posts to start to rectify that. You may wish to read &lt;a href=&quot;/2011/12/how-im-testing-iphone-apps-part-1&quot;&gt;part 1&lt;/a&gt; first.&lt;/i&gt;&lt;/p&gt;

## Kiwi

We were looking for a testing framework which supported iOS's asynchronous programming model and [Kiwi](https://github.com/allending/Kiwi) answered the call. It has a great syntax, [comprehensive set up assistance](https://github.com/allending/Kiwi/wiki/Guide:-Up-and-Running-with-Kiwi), asynchronous support and built in mocking. I'd highly recommend you check it out: the syntax helps me to think in the right way and it has pretty much all the features we needed.

Kiwi's block syntax looks like this:

{% highlight objectivec %}
describe(@&quot;Team&quot;, ^{
    context(@&quot;when newly created&quot;, ^{
        it(@&quot;should have a name&quot;, ^{
            id team = [Team team];
            [[team.name should] equal:@&quot;Black Hawks&quot;];
        });
    });
});
{% endhighlight %}

Much better than the old fashioned xUnit style of testing, in my opinion. You might hate it, of course. You can use Kiwi's features [without having to use the block syntax](https://github.com/allending/Kiwi/issues/73) if you want.

## Objective-C's delegate model

Many of the Apple core libraries use a delegate pattern for handling callbacks from a class. This is similar to Java's interfaces, and superficially similar to blocks in Ruby and anonymous functions in Javascript.

As an example, let's take CoreLocation. When wanting to find the location of a phone, you create a new `CoreLocationManager` and call `startUpdatingLocation` on it:

{% highlight objectivec %}
CLLocationManager *manager = [[CLLocationManager alloc] init];
[manager startUpdatingLocation];
{% endhighlight %}

This call returns immediately: so how do you execute code when the location is found? You use a delegate: an object with responds to the `locationManager: didUpdateToLocation: fromLocation` method:

{% highlight objectivec %}
-(void)locationManager:(CLLocationManager *)manager didUpdateToLocation:(CLLocation *)newLocation fromLocation:(CLLocation *)oldLocation
{
  NSLog(&quot;$@ I AM IN YOU&quot;, newLocation);
  foundLocation = YES;
}
{% endhighlight %}

Then you set this object to be the CLLocationManager's delegate before calling `startUpdatingLocation`. Often you set the delegate to `self` and define the delegate method on the calling object.

{% highlight objectivec %}
CLLocationManager *manager = [[CLLocationManager alloc] init];
manager.delegate = self;
[manager startUpdatingLocation];
{% endhighlight %}

There's more about this model in [this article from Apple](http://developer.apple.com/library/IOs/#documentation/iPhone/Conceptual/iPhone101/Articles/02_DesignPatterns.html).

## Testing delegates

This is tricky to test, because we can't simply do this:

{% highlight objectivec %}
it(&quot;should call the delegate when ready&quot;, ^{
  [testObject startUpdatingLocation];
  [[testObject.foundLocation should] equal:theValue(YES)];
});
{% endhighlight %}

The test will call `startUpdatingLocation`, and then immediately check the `foundLocation` property to see whether it's been set. It won't have been, because the delegate won't have been called yet.

How were we to stub endpoints such as the location system for for our app? We found two ways of doing this, with varying effectiveness:

* Using Objective-C categories to redefine class methods
* Using a Kiwi stub to inject a derived class which mocks out key methods

Next post, I'll dive into some detail on both of these methods and show some of the pros and cons of each.

&lt;i&gt;How are you testing iPhone apps? Do chime in throughout the series with suggestions and comments, and I'll edit the posts as appropriate.&lt;/i&gt;

</content></entry><entry><title>How I'm testing iPhone apps: part 1</title><category term='code'/><category term='ios'/><category term='tdd'/><category term='bdd'/><category term='testing ios'/><link href='http://chrismdp.com/2011/12/how-im-testing-iphone-apps-part-1'/><updated>2011-12-01T22:45:35+00:00</updated><id>http://chrismdp.com/2011/12/how-im-testing-iphone-apps-part-1</id><content type='html'>&lt;p&gt;&lt;i&gt;This week I've been working with &lt;a href='http://shilling.co.uk'&gt;Shilling&lt;/a&gt; helping them get starting with iOS application development. Part of the deal was for me to learn it myself as we went: I've done hardly any iOS work and we've been learning how to do it together.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;As part of this process, working out the best way to test-drive the development of iOS apps was high on my priority list. I know that the automated testing of iOS applications is still not widely practiced and isn't well documented, so I decided to write a series of posts to start to rectify that.&lt;/i&gt;&lt;/p&gt;

## Our goal

There are two main parts to working out how to test-drive applications on a new platform. One is to figure out the testing libraries and write simple `1 + 1 = 2` style tests to prove it can be done. The other half is working out how to apply common testing techniques such as stubbing external systems, isolating tests correctly and optionally driving the interface.

The first of these steps is quite easy on iOS, but the second part is harder. In our case, we have some code which makes use of CoreLocation and the [Geonames](http://geonames.org) service to get an iPhone's location and look up the county name from a latitude and longitude. This means that our code relies on two external services to run, which we want to stub out: we don't want these services to be called each time our tests run. How were we to set this up correctly?


## Apple's documentation

To kick off our testing adventure on iOS, we started with [Apple's own public documentation](http://developer.apple.com/library/ios/#documentation/Xcode/Conceptual/ios_development_workflow/135-Unit_Testing_Applications/unit_testing_applications.html) on how to test iOS. This is a fairly comprehensive guide on how to set up a project with built in testing, allowing you to write basic SenTest tests quite quickly.

Apple divides its definition of unit testing into two categories:

* Logic tests: these are what I would normally call unit tests. They rely on very few external APIs and are run standalone without the use of a simulator.
* Application tests: these are executed in the context of a running application on a simulator or iOS device.

The document details how to set up both types in your project. There's a few things missing though:

* They have good ideas about [how to write decent tests](http://developer.apple.com/library/ios/#documentation/Xcode/Conceptual/ios_development_workflow/135-Unit_Testing_Applications/unit_testing_applications.html#//apple_ref/doc/uid/TP40007959-CH20-SW12), but lack information on how to correctly mock system endpoints. I want to do this so that I don't have to rely on iOS location simulation, or HTTP response data, to make my tests work.
* There was also nothing on how to test asynchronously, which is a real problem as iOS applications are mostly written in this way.
* Application tests are executed in the context of your application, but without extra work it's not possible to support native UI testing, [Capybara](https://github.com/jnicklas/capybara) style. We are reduced to manipulating controllers directly, which is good enough for now. This assumes you have your user interface wired up correctly. As the app always has to be tested manually anyway then this isn't too much of a risk, but if you want to take a step further you could use [KIF](https://github.com/square/KIF), [Frank](https://github.com/moredip/Frank) or Apple's own [UIAutomation](http://developer.apple.com/library/ios/#documentation/DeveloperTools/Reference/UIAutomationRef/Introduction/Introduction.html). There's a good post comparing them [here](http://sgleadow.github.com/blog/2011/10/26/which-automated-ios-testing-tool-to-use/).

So we followed through the basic set up instructions, and got a simple test running which added two numbers together. A good start, but useless for real work.

Time to go in search of an asynchronous testing framework: and we found a great one. Next time, I'll talk about the wonderful [Kiwi](https://github.com/allending/Kiwi).

&lt;i&gt;How are you testing iPhone apps? Do chime in throughout the series with suggestions and comments, and I'll edit the posts as appropriate.&lt;/i&gt;
</content></entry><entry><title>Work with me</title><category term='code'/><category term='meta'/><category term='agile'/><category term='training'/><category term='life'/><category term='tdd'/><link href='http://chrismdp.com/2011/08/work-with-me'/><updated>2011-08-22T15:36:53+01:00</updated><id>http://chrismdp.com/2011/08/work-with-me</id><content type='html'>I've been taking a break from my work and blogging for the summer: but now I'm looking for work again from next week.

To this end I've put up a [new page](/workwithme.html) on this site which has all the details of what I can offer, and my availability. If you'd like to work with me this autumn, do [get in touch](mailto:chris@thinkcodelearn.com).

I also plan to start blogging again soon, with my first topic being [how we handled the massive site traffic](/2011/08/e-petitions-handling-traffic) we experienced on e-petitions [a couple of weeks ago](http://www.bbc.co.uk/news/uk-politics-14474429).
</content></entry><entry><title>Pomodoros help you refactor</title><category term='code'/><category term='pomodoros'/><category term='tdd'/><category term='craftsmanship'/><category term='pairing'/><link href='http://chrismdp.com/2011/04/pomodoros-help-you-refactor'/><updated>2011-04-01T20:21:15+01:00</updated><id>http://chrismdp.com/2011/04/pomodoros-help-you-refactor</id><content type='html'>&lt;p&gt;&lt;i&gt;&quot;If you finish a task while the Pomodoro is still ticking, the following rule applies: If a Pomodoro Begins, It Has to Ring. It’s a good idea to take advantage of the opportunity for overlearning, using the remaining portion of the Pomodoro to review or repeat what you’ve done, make small improvements, and note down what you’ve learned until the Pomodoro rings.&quot;&lt;/i&gt;&lt;/p&gt;

-- Francesco Cirillo, [The Pomodoro Technique](http://www.pomodorotechnique.com/)

What's the single most important part of Test Driven Development not to miss? Refactoring. What's the part of TDD that's most often missed? Refactoring.

With refactoring, we work our way toward a great design, clean code, and flexible organic tests. Without refactoring, we have ugly brittle test suites and uglier code. We know this. What I don't always do is take advantage of the moments I have when I can effectively refactor for free.

At the end of a task, when the build is running, I've previously let my mind wander to the next thing, or check email, surf the net, and generally [get out of the zone](http://www.computus.org/journal/?p=982). This bad habit has been highlighted to me in [my use of the pomodoro technique recently](/2011/03/pomodoros-done-hopefully-right).

I was doing the same for the shorter pauses during normal TDD. My pomodoros statistics were telling me that I'm very bad at concentrating whilst coding: the average time spent before I let my mind wander was 11.67 minutes. I was allowing my mind to drift whilst Rails started up to run whatever test I was working on. Not good.

## Time to improve

This week, I've been trying to take the time to look at my code critically for areas of improvement. A pomodoro is indivisible, which means I'm not _allowed_ to think about anything else.

And guess what? I always find something to improve, and I feel that little bit better about my code.

The also helps with the thing I've missed most about not pairing: that other person's critical eye on what you're doing, always thinking about the code being written. During the natural pauses, you can be that other person and ensure the code you write is great. Being two people is [more fun, too](http://www.pixar.com/shorts/gg/index.html).
</content></entry><entry><title>How to test your node.js app</title><category term='javascript'/><category term='code'/><category term='tdd'/><category term='node.js'/><link href='http://chrismdp.com/2010/05/tdd-with-node-js'/><updated>2010-05-23T11:05:00+01:00</updated><id>http://chrismdp.com/2010/05/tdd-with-node-js</id><content type='html'>I've wanted to hack on a [node.js](http://nodejs.org) project for a while, and a new app idea has given me the perfect excuse. My first question was: how do I test this? It's a fairly new field out there, and there isn't much help from node.js itself: it's much more like [Rack](http://rack.rubyforge.org/) than a proper framework. So I spent some time coming up with one way to do it.

Disclaimer: I'm not that experienced with Javascript, particularly with the best way to define objects. I'd be grateful for patches to help improve the quality of the code here. I've also borrowed heavily from [apprentice-us](http://github.com/redsquirrel/apprentice-us) - thanks to [Dave](http://twitter.com/redsquirrel) and [Corey](http://twitter.com/coreyhaines)!

## Overview 


This is what I've got so far (the actual app I'm working will remain closed-source for the mo):

[Example node.js github project](http://github.com/chrismdp/example-nodejs-project)

You probably want to refer to the code whilst reading the rest of this article.

To run the tests, run _rake_. To start the app, run _node app.js_ (you will need to have node.js installed obviously).

If you install the [watchr](http://github.com/mynyml/watchr) gem, and run _watchr autotest.watchr_, you'll get robust autotest like functionality. I'm liking watchr much better than ZenTest right now.

## How it works

The basic premise is to decouple the request/response handler from the server (see *app.js*, *lib/http.js* and *lib/router.js*). The interesting bit is in *test/ integration/ user_sees_homepage.js* - this then calls the dispatch method directly, passing in dummy Request and Response objects.

Note how I've [defined the Response object](http://github.com/chrismdp/example-nodejs-project/blob/master/test/integration/user_sees_homepage.js). This allows me currently to write an integration test that looks like this:

{% highlight javascript %}
router.dispatch(new Request(&quot;GET&quot;, &quot;/&quot;), new Response(function(headers, data) {
  assert.contains(&quot;200&quot;, headers['status'])
  assert.contains(&quot;Hello, world!&quot;, data)
}));
{% endhighlight %}

The assert.contains() method is not part of node.js: it's implemented in _test/helper.js_.

The reason you need the asserts to be fired in the end() function is that node.js is inherently asynchronous and will finish executing this file whilst waiting for the haml file to load in *lib/router.js*. Try it yourself: if you put an assert at the bottom of the file it will fire immediately.

## Unit tests

The plan is then to define whichever unit tests you need in *test/ unit/ (something)_test.js*, with the corresponding code in *lib/ models/ (something).js*. Just run javascript code in here and call methods on assert, and rake will execute it.

## Improvements

You could potentially use the Sinatra-like framework [Express](http://expressjs.com) to define *lib/router.js* - I've handrolled it for the moment. I'm of the opinion that you spot betterrefactorings by handrolling to start with: it could be that express.js isn't right for my app, and I can't easily tell yet.

There are a number of javascript testing libraries out there, but at the moment I'm happy with my own handrolled version, which just relies on the 'assert' package that node.js provides. There's nothing to stop you using JSpec or some other javascript testing library: I wanted to keep things simple to start with. 

I'm also aware that Cucumber [now supports javascript through V8](http://blog.josephwilk.net/ruby/testing-javascript-with-cucumber-in-javascript.html), which is an important step in the right direction. Unfortunately however it doesn't yet support the [commonjs](http://commonjs.org) package system, and doesn't run through node.js but through raw V8. This makes it hard to use with anything but toy examples. Ideally I've love to plug Cucumber in in the future, if we can get it to use node.js as the platform somehow.

If you use it for something useful, let me know! I'd be very happy to receive patches and suggestions.
</content></entry><entry><title>TDD ~ The Tortoise and the Hare</title><category term='tdd'/><link href='http://chrismdp.com/2006/02/tdd-the-tortoise-and-the-hare'/><updated>2006-02-16T09:43:00+00:00</updated><id>http://chrismdp.com/2006/02/tdd-the-tortoise-and-the-hare</id><content type='html'>&lt;div class='notice'&gt;&lt;h1&gt;By the way...&lt;/h1&gt;&lt;p&gt;This content is now pretty old: check &lt;a href='/'&gt;the homepage&lt;/a&gt; for the latest.&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;Test Driven Development. Some people just don't get it.&lt;/p&gt;
&lt;p&gt;Some people say: &quot;A test for each feature? But that would mean writing twice as much code!&quot;&lt;/p&gt;
&lt;p&gt;These people usually go on to say: &quot;It's just not worth it. I've finished my code, and it works, and you're still writing the test! I leave you in dust, 
you slow coding type!&quot;&lt;/p&gt;
&lt;p&gt;Then again, some people choose platforms, frameworks and even programming languages based on how easy it is to do TDD using them. If they can't do TDD ea
sily, they pick something else. They're that committed to it.&lt;/p&gt;
&lt;p&gt;The hare starts out faster. The tortoise is much slower out of the gate. &lt;/p&gt;
&lt;p&gt;The tortoise knows that the finish line isn't when the code is written, it's when the customer has a working product. The hare might think they've finish
ed, so they sit down and have a nap (or to stretch the analogy slightly, get bogged down fixing unnecessary bugs). That's when the tortoise overtakes.&lt;/p&gt;
&lt;p&gt;I used to be a hare, running frantically back and forth over the whole codebase, changing things here and there until I thought I was done with a feature
. Invariably I wasn't; there were bugs to be fixed and features missed. I thought just because I could write code quickly, I was adding value quickly.&lt;/p&gt;
&lt;p&gt;And then I fell in love with TDD. Since I discovered it after reading &quot;Extreme Programming explained&quot;  by Kent Beck (buy: &lt;a href=&quot;http://www.amazon.co.u
k/exec/obidos/redirect?link_code=ur2&amp;amp;tag=chrisparsonbl-21&amp;amp;camp=1634&amp;amp;creative=6738&amp;amp;path=ASIN%2F0201616416&quot;&gt;uk&lt;/a&gt;   &lt;a href=&quot;http://www.amazo
n.co.uk/exec/obidos/redirect?link_code=ur2&amp;amp;tag=chrisparsonbl-21&amp;amp;camp=1634&amp;amp;creative=6738&amp;amp;path=http%3A%2F%2Fwww.amazon.com%2Fgp%2Fproduct%2F03
21278658%2Fref%3Dpd_kar_gw_1%3F%255Fencoding%3DUTF8%252CUTF8%26ref%3Dpd%255Fkar%255Fgw%255F1%26v%3Dglance%26n%3D283155&quot;&gt;us&lt;/a&gt;), I've not looked back. All o
f my clients (like it or not), suddenly got test frameworks for free just so that I could develop using TDD. I rarely do any coding at all in any other way.
&lt;/p&gt;
&lt;p&gt;Why use TDD? Here are my main reasons:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Refactoring power.&lt;/em&gt; If I have a unit test testing each feature of my code module, I can merrily move stuff around internally as much as I like, 
without worrying if it's broken. At the end, I just run the tests. If it still works, I'm done.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Remembering what you are doing.&lt;/em&gt; It's very easy in a large application to get lost on which feature you are currently coding. If you are coding 
using TDD, then you can't get lost. There's a very simple cycle to follow: Write failing test. Write code so test passes. Refactor if necessary. If you forg
et where you are, just run the tests. If they pass, you need to move to a new feature. If they fail, fix them. Simple.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Peace of mind.&lt;/em&gt; I've written a large application recently which bills customers weekly for products. When I release a new version of the applica
tion, I &lt;i&gt;have&lt;/i&gt; to know the billing is going to work. If it doesn't I get in a lot of trouble. Now I know the application is working before I release it
, just by running &quot;ant test&quot; and waiting a couple of minutes.&lt;/p&gt;
&lt;p&gt;Hares start out faster. But they get complacent, they have a nap, and invariably run into problems in the end. Tortoises get to the finish line first.&lt;/p
&gt;
&lt;p&gt;If you'd like to know more about TDD, visit these links:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.agiledata.org/essays/tdd.html&quot;&gt;A nice TDD essay&lt;/a&gt;, comparing TDD to MDD (model driven development).&lt;br /&gt;
&lt;a href=&quot;http://www.jamesshore.com/Blog/Microsoft-Gets-TDD-Completely-Wrong.html&quot;&gt;A rebuttal&lt;/a&gt; of Microsoft's version of TDD, with a very good intro to 'real' TDD, by &lt;a href=&quot;http://www.jamesshore.com/&quot;&gt;James Shore&lt;/a&gt;.&lt;/p&gt; 
</content></entry></feed>
