---
layout: nil
---
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Chris Parsons: posts tagged adn</title>

  <link href="http://chrismdp.com/"/>
  <updated>2011-08-30T13:51:38+01:00</updated>
  <id>http://chrismdp.com/tag/adn</id>
  <author>
    <name>Chris Parsons</name>
    <email>chrismdp@gmail.com</email>
  </author>
<entry><title>e-petitions: handling traffic</title><category term='code'/><category term='scaling'/><category term='agile'/><category term='adn'/><category term='e-petitions'/><category term='government'/><link href='http://chrismdp.com/2011/08/e-petitions-handling-traffic'/><updated>2011-08-30T13:51:38+01:00</updated><id>http://chrismdp.com/2011/08/e-petitions-handling-traffic</id><content type='html'>Since I [last blogged about e-petitions](/2011/07/e-petitions-deconstructed) we had what conservatively could be described as &quot;something of a traffic spike&quot;. The [amount of interest](http://www.bbc.co.uk/news/uk-politics-14474429) surrounding the site massively exceeded all our expectations.

Given the time available to us, we had rated the site for about 10 requests a second, basing our expectations on 50% more usage than the original e-petitions site. However, during peak times we were seeing non-cached bursts of traffic through to the site of between 70-120 requests a second: we'd load tested up to about 40-50. This caused the site to intermittently produce [500 errors](http://epetitions.direct.gov.uk/500.html); this in turn [producing](http://www.thesun.co.uk/sol/homepage/news/3733792/E-petitions-website-down-on-first-day.html) [headlines](http://www.guardian.co.uk/politics/2011/aug/04/government-e-petition-website-crashes) that we really didn't want to see.

Most perplexingly, we were still seeing intermittent failure messages in the logs when the site was getting about 20-30 requests a second, even though that had worked fine in testing. During this time none of the servers were under a huge amount of load, so we struggled to find the bottleneck.

Eventually we discovered that the hardware firewall we'd put in place to help improve security wasn't able to handle the number of network connections required of it, and was randomly dropping network connections. This included connections on the internal network, which caused connectivity problems to the seach and database servers. This caused most of the intermittent failures people saw. The firewall had been set up after we had done our load testing and we'd not re-run our testing since then, so we hadn't spotted the problem
.

Once this had been fixed, we were on to more familiar territory. The dedicated solr server we were running for search was really struggling with only 4 CPUs, so we rebooted it using 8 CPUs and it started working much better.

We also made a number of other changes to the site to make it more robust:

* We brought an application server down, cloned the disk and set up a third application server within about half an hour. It's not as quick as running on Amazon EC2, but it's not a bad turnaround for a more traditionally hosted site.

* We set up monitoring on the site using [Munin](http://munin-monitoring.org/), which is a brilliant server montoring tool. This helped us discover the solr issues much more quickly.

* We went right through the code and turned on caching everywhere we hadn't yet thought of, including caching of more pages surrounding the signing step.

* We worked around a sunspot issue which was causing a petition to reindex after every signature, stressing the search server further.

## Lessons learnt

* Run your load tests again after any configuration change, even if it shouldn't make a difference. If we'd done this, we'd have spotted the firewall configuration issue before the public did.
* Set up proper measuring tools before the event. It took us a while to find the best cause of action with the search server because we were relying on [New Relic](http://newrelic.com) to monitor the application servers only. Once we had Munin running we could more easily make more CPUs available to the search server.

The vast majority of the above changes were made within one (very late) night: from a customer point of view, being agile isn't just about how flexible you are during development, but how responsive you can be when there are problems. Our technology and stack choice really helped us out here, and I also particularly wanted to thank the Alpha.gov guys ([Ben Griffiths](http://twitter.com/beng), [James Stewart](http://twitter.com/jystewart) and [Matt Patterson](http://twitter.com/fidothe)) and [Alex Tomlins](http://www.unboxedconsulting.com/people/alex-tomlins) from [Unboxed](http://unboxedconsulting.com) for stepping in and helping [Jolyon](http://www.unboxedconsulting.com/people/jolyon-pawlyn) and myself out that evening.

Jolyon and I are [giving a talk about e-petitions at LRUG](http://lanyrd.com/2011/lrug-september/sgzxr/) next month if you'd like to hear more.
</content></entry><entry><title>e-petitions: deconstructed</title><category term='code'/><category term='chef'/><category term='agile'/><category term='adn'/><category term='government'/><category term='e-petitions'/><link href='http://chrismdp.com/2011/07/e-petitions-deconstructed'/><updated>2011-07-29T15:57:09+01:00</updated><id>http://chrismdp.com/2011/07/e-petitions-deconstructed</id><content type='html'>&lt;div class='notice'&gt;Update: I've posted more about the massive traffic surge and how we responded &lt;a href='/2011/08/e-petitions-handling-traffic'&gt;here&lt;/a&gt;.&lt;/div&gt;

The project that I've been working on at the Government Digital Service (GDS) for the last few weeks has just been launched. It's the new Government [e-petitions](http://epetitions.direct.gov.uk) service, which replaces the old Number 10 petitions website run by the previous government. Time to talk about the architecture, how we set the team up and the effect the project is having within government.

![e-petitions](/files/e-petitions.png)

## The project

The project was overseen and run by [Skunkworks](http://twitter.com/HMGSkunks), the new innovation arm of the GDS that specialises in quick projects with small teams. They hired the [Agile Delivery Network](http://agiledelivery.net) (ADN) to do the work: this is a non-profit organisation I'm involved with that's trying to help government deliver IT projects more quickly.

We put together a team consisting of myself and two other developers, a designer, a tester/project manager, our customer and an analyst to help with the copy and training the staff who will be moderating petitions.

We originally started the project at the very beginning of June, knowing that we only had six weeks to get the site live. We ran three two-week iterations, during which requirements shifted around as the important deliverables came into focus.

There were a number of major technical hurdles. We spent a lot of time getting the accessibility of the site right, and tweaking the feel of the search feature. Getting the site hosted was difficult: it's not straightforward finding website hosting for a government website that collects personal data.

Whilst we tried to find the right place to host the site, we spent a lot of time using [Chef](http://www.opscode.com/chef) to test our build configuration on Amazon EC2. When the hosting came online, it was relatively simple (thankfully) to deploy the site to the production environment, as we'd already prepared all the configuration scripts in advance.

## The tech

We built the site in Rails, with a MySQL and a Solr search backend. It's running in production on two application servers, through nginx for static content with unicorn at the backend. There is one dedicated DB server, and one dedicated Solr server. Our JMeter testing showed that we may not need the dedicated Solr server, so that might also share CPU with a read-only MySQL slave in future if the site traffic gets heavier.

For server configuration, we're running a customised version of chef-solo on each of the servers, bootstrapped with a little bespoke script. We didn't want to set up a chef server as we didn't get the hosting environment set up until quite late in the day, and we didn't want an external server with access to the production environment.

Chef turned out to be awesome: it was very satisfying to watch all the scripts we'd built on EC2 &quot;just work&quot; (well, almost) on the live environment. Nginx + Unicorn was also a highlight: it's more Unix-y that Apache + Passenger and handles graceful restarting very nicely.

## The reaction

The site has been well received by those outside government, but perhaps just as importantly the way that we ran the project caused a bit of a stir within Whitehall too. Agile projects are still rare in government, and IT spending is a [hot topic](http://www.bbc.co.uk/news/uk-politics-14314935) right now. It's great that people are beginning to think about how to deliver software in better ways and the guys at Skunkworks are doing really well at promoting agile methods internally.

## The team

Everyone who worked hard to make the site what it is: it was great fun working with you!

* [Tom Dickinson](http://www.unboxedconsulting.com/people/tom-dickinson) from [Unboxed](http://unboxedconsulting.com)
* [Peter Herlihy](http://uk.linkedin.com/in/peterherlihy)
* [Charlie MacLoughlin](http://uk.linkedin.com/pub/charlie-macloughlin/3/183/821)
* [Jolyon Pawlyn](http://www.unboxedconsulting.com/people/jolyon-pawlyn) from [Unboxed](http://unboxedconsulting.com)
* [Alan Thomas](http://www.unboxedconsulting.com/people/alan-thomas) from [Unboxed](http://unboxedconsulting.com)
* [Will Tomlins](http://www.unboxedconsulting.com/people/will-tomlins) from [Unboxed](http://unboxedconsulting.com)
* Me

We're planning to get the code out on github soon. Hope you like the site and enjoy using it.
</content></entry></feed>
