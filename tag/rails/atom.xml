---
layout: nil
---
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Chris Parsons: posts tagged rails</title>

  <link href="http://chrismdp.com/"/>
  <updated>2011-10-17T19:10:29+01:00</updated>
  <id>http://chrismdp.com/tag/rails</id>
  <author>
    <name>Chris Parsons</name>
    <email>chrismdp@gmail.com</email>
  </author>
<entry><title>Your tests are lying to you</title><category term='code'/><category term='cucumber'/><category term='craftsmanship'/><category term='bdd'/><category term='rspec'/><category term='rails'/><link href='http://chrismdp.com/2011/10/your-tests-are-lying-to-you'/><updated>2011-10-17T19:10:29+01:00</updated><id>http://chrismdp.com/2011/10/your-tests-are-lying-to-you</id><content type='html'>Using mocks within your test suite has gone rather out of fashion. Programmers everywhere have been lamenting the fact that mock-based tests are becoming more and more brittle: they're having to change the test code in multiple places each time there's the slightest code change. In fact, they seem to be changing the test code much much more often than the production code.

Using mocks appear to require a lot of set up code for the object under test. Why not just fire up Factory Girl, create a bunch of objects we need to test this code, and just check the outputs?

This works, and appears to work nicely. For a while.

Eventually your tests will get to the point where they're lying to you: they're telling you your code works whereas actually it only works by coincidence. This post will examine the different techniques we can use to test code, and why some work better than others in the long term.

## The problem

To look at this further, let's try to write a conference simulator for a new website that tries to predict how many people might attend an upcoming event: 

{% highlight ruby %}
describe Conference do
  it &quot;calculates total rating&quot; do
    conference = Conference.new(:total_rating =&gt; 9)
    conference.total_rating.should == 9
  end
end
{% endhighlight %}

A simple start, with equally simple production code. Next, we decide to extract our code for calculating the rating into &lt;code&gt;Speaker&lt;/code&gt; classes. We decide not to change the test suite much, and make the code work behind the scenes:

{% highlight ruby %}
describe Conference do
  it &quot;calculates total rating&quot; do
    conference = Conference.new(:speakers =&gt; [:chris, :paul])
    conference.total_rating.should == 9
  end
end
{% endhighlight %}

A nice simple, easy change? You'll pay for this later. Where is the Speaker coming from? Your Conference class is creating it somewhere, or retrieving it from a factory. You've increased the number of collaborators for this class by at least one (possibly three), yet your test isn't showing the additional complexity. It's deceitfully hiding it, whilst you continue on in blissful ignorance.

Your tests are now sitting around the outside of your system. There are no tests for the Speaker class at all, except that we co-incidentally check the rating it emits. Another developer is likely to miss the connection and remove the implied test whilst changing the code for a different reason later.

This gets worse over time:

{% highlight ruby %}
describe Conference do
  it &quot;calculates total rating&quot; do
    conference = Conference.new(
      :schedule =&gt; :nine_to_five,
      :talks =&gt; [talk_for(:chris), talk_for(:paul)]
    )
    conference.total_rating.should == 9
  end
end
{% endhighlight %}

Can you see what's going on here? We've created some nice helper methods to make it easy to create the required talk objects we need. This test is fairly easy to read, but it's dressing up the problem. The test code is relying on far too many collaborators to function correctly to return the correct result.

When you extract a class, your purely state based tests don't always require change. If you're not stubbing out or mocking systems, you can end up in a situation where you're relying on the code to work without realising it.

How could it be improved?

{% highlight ruby %}
describe Conference do
  let(:talk1) { double(:talk, :rating =&gt; 10) }
  let(:talk2) { double(:talk, :rating =&gt; 6) }
  let(:schedule) { double(:schedule, :rating =&gt; 10) }
  before(:each) { Schedule.stub(:new =&gt; schedule) }
  it &quot;calculates total rating&quot; do
    conference = Conference.new(
      :schedule =&gt; :nine_to_five,
      :talks =&gt; [talk1, talk2]
    )
    conference.total_rating.should == 9
  end
end

describe Speaker do
end
describe Schedule do
end
{% endhighlight %}

Now we've isolated the method nicely from its collaborators, and ensured that its behaviour is correct: that it aggregates the ratings of the talks and the schedule. We also make sure that we're testing Conference correctly, also in isolation.

The more you use refactoring methods such as Extract Class without cleaning up your tests, the more likely your tests will be lying to you. Little by little, those tests that you trusted are slowly testing more and more code. You add a multitude of edge cases at the edges, never thinking about the complexity within. You've resorted to using end-to-end tests to test basic correctness.

This is a bad thing on many levels: for example, what happens to interface discovery? How will you know how the interface of your lower-level classes needs to behave if you're not mocking or stubbing it? You are resorting to guessing, rather than exercising the interface ahead of time in your tests. If you have tests around the edges, but not in the middle, you're not gaining the design input that tests give you in each layer of your system.

## Your code stinks

If you go the whole hog with testing in isolation, then you might end up here with something like this:

{% highlight ruby %}
describe Conference do
  let(:talk1) { double(:talk, :rating =&gt; 10) }
  let(:talk2) { double(:talk, :rating =&gt; 6) }
  let(:talk3) { double(:talk, :rating =&gt; 2) }
  let(:talk4) { double(:talk, :rating =&gt; 8) }
  let(:track1) { double(:track, :talks =&gt; [talk1, talk3] }
  let(:track2) { double(:track, :talks =&gt; [talk2, talk4] }

  let(:venue1) { double(:venue, :nice_coffee_places =&gt; 3) }

  let(:joe) { double(:announcer, :experience =&gt; 5) }

  let(:schedule) { double(:schedule, :rating =&gt; 10, :accouncer =&gt; joe) }
  before(:each) { Schedule.stub(:new =&gt; schedule) }

  it &quot;calculates total rating&quot; do
    conference = Conference.new(
      :schedule =&gt; :nine_to_five,
      :tracks =&gt; [track1, track2],
      :organiser =&gt; joe,
      :venues =&gt; [venue1, venue1]
    )
    conference.total_rating.should == 6.3945820
  end
end

{% endhighlight %}

I'm not surprised people moan about maintaining this: if any aspect of the Conference class changes, this test will break and need to be fixed. We can see that this test code is hard to write and difficult to read. It would be so much easier just to hide this setup in a few factory methods with some sensible defaults, right?

Maybe it's not the test code that's the problem. Perhaps the code stinks. Perhaps the class simply has way too many collaborators, which is why your test code contains a large amount of set up.

For this test code, we can see there are several objects leaking all over the conference code: to refactor this I'd probably get through a Scheduler, Caterer and perhaps a TrackAggregator before I was done. I'd ensure all these objects were tested in isolation, and ensure that there are acceptance tests all the way through to make sure the customer has what they need.

_Well designed code is easy to test._ As a rule of thumb, anytime I get over about two or three lines of setup code for testing a method, I normally take a step back and ask myself if this method is doing too much.


## Test speed

The other advantage of running tests purely in isolation is that they're fast. Very fast. When I'm coding Rails apps these days, thanks to advice from [Corey Haines](http://twitter.com/coreyhaines) I'm running a &lt;code&gt;spec_no_rails&lt;/code&gt; folder which runs independently from the rest of my Rails app. Rails apps by default epitomise this problem: default model tests exercise the whole system from the database up. By running your tests independently you're not having to clean the database or start Rails each time you run your tests, which means that much of your interesting code can be tested in under a second. [Gary Bernhardt](http://twitter.com/garybernhardt) has more information on how to set this up in his excellent [Destroy All Software](http://destroyallsoftware.com) screencast series.

## What I'm not saying

This isn't an argument for or against Mocks or Stubs. Either technique can be used successfully to generate clean code. It's an argument about only exercising the code under test, and leave the rest of the system to take care of itself. The important thing is that you _don't exercise your collaborators:_ whether you check they've received messages or simply stub them to return input doesn't matter.

*Don't forget end-to-end tests.* These are very important for business acceptance and for ensuring basic functionality. The important thing is to ensure that you're being intentional about your end-to-end tests and ensure your unit tests are not end-to-end tests by accident.

Take a good look at the test code for a project you recently worked on. You don't need to look at the production code yet: notice that I've not included any production code in these examples. You shouldn't need to see it to know whether it's of good quality or not: you can tell that by reading the tests.

Which is the most annoying or bulky part of your test code? Are your tests deceiving you about what they're testing? How could you improve the code to make this test code easier to maintain?
</content></entry><entry><title>Kanogo: vapourware to beta in 24 hours</title><category term='code'/><category term='products'/><category term='business'/><category term='kano analysis'/><category term='rails'/><category term='heroku'/><category term='ruby'/><category term='kanogo'/><link href='http://chrismdp.com/2011/09/kanogo-vapourware-to-beta-in-24-hours'/><updated>2011-09-12T11:30:37+01:00</updated><id>http://chrismdp.com/2011/09/kanogo-vapourware-to-beta-in-24-hours</id><content type='html'>&lt;div class='notice'&gt;
  &lt;h2&gt;TL;DR&lt;/h2&gt;

  &lt;p&gt;Last week I built the first beta of a new web product called &lt;a href=&quot;http://kanogo.com&quot;&gt;Kanogo&lt;/a&gt;. It’s designed to gather feedback and perform &lt;a href=&quot;http://en.wikipedia.org/wiki/Kano_model&quot;&gt;Kano analysis&lt;/a&gt; to determine which direction you should take with your website.&lt;/p&gt;

  &lt;p&gt;Here's an example, designed specifically for this blog. Thanks for your feedback!&lt;/p&gt;

  &lt;iframe allowtransparency='true' frameborder='0' scrolling='no' src='http://kanogo.com/surveys/13/embed?' style='width: 100%; height: 120px'&gt;
  &lt;/iframe&gt;

  &lt;p&gt;Sign up for the beta &lt;a href='http://kanogo.com'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;

## The backstory

A while back I agonising over which should be the next greatest feature for one of my products. I thought the best thing to do would be to conduct some Kano analysis on the product in question, and realised there wasn't an easy way of doing this. I've used [kanosurvey.com](http://kanosurvey.com) in the past, but it didn't really feel like the right tool. How was I to get users to answer my survey?

&quot;Wouldn't it be great,&quot; I thought, &quot;if I could embed a little survey box on the site that asked customers what they thought and provided me with Kano analysis stats?&quot; The concept behind [Kanogo](http://kanogo.com) was born.

Fast forward several months to last week. I found myself with a few days spare and decided that the best use of them would be to build a beta of this product. Always up for a challenge, I decided to give myself 24 hours to build and launch.

That's not very long, so I had to hustle.

## Timeline

*7 Sep: 12:10am:* [I announced my intentions](https://twitter.com/#!/chrismdp/status/111214768651636736), mostly to motivate myself through fear of failing in public. I finally decided on a name, and registered the domain and the twitter account. I announced the product [to the world](https://twitter.com/#!/chrismdp/status/111240345341263872) (well, a [subset](https://twitter.com/#!/chrismdp/followers)).

*7 Sep: 01:55am:* Got a new Rails 3.1 app running on Heroku cedar. It's a one page app using a Campaign Monitor signup form. Got my first beta signup. Finished for the night.

*7 Sep: 07:40am:* Announced Kanogo again, just in case anyone had been sleeping at 2am :) Got another 3 beta signups and a bunch of feedback on spelling errors.

*7 Sep: 10:13am:* Simple twitter sign in done using [Omniauth](https://github.com/intridea/omniauth) and this really useful [tutorial](https://github.com/RailsApps/rails3-mongoid-omniauth/wiki/Tutorial).

*7 Sep: 02:45pm:* The USA woke up and I got more beta signups: now up to 5. Got the basic data entry for surveys and features done. Started work on the embed. Was feeling fairly pessimistic about a beta launch for that night, but didn't want to let myself down.

*7 Sep: 05:53pm:* Embed done, quicker than expected. Took a break. Now feeling [cautiously optimistic](https://twitter.com/#!/chrismdp/status/111482135218626560).

*7 Sep: 09:12pm:* Basic response mechanism in: now needed to apply the Kano analysis magic! Adrenalin took over from caffiene as primary stimulant.

*7 Sep: 11:20pm:* Turned on twitter sign in as basic method of getting registered on the site. Removed redundant Campaign Monitor signup: emailed subscribers manually to ask them to sign in via twitter. Beta [went live!](https://twitter.com/#!/kanogoapp/status/111564545708929024)

## The result

![Embed](/files/kanogo-1.png)

![Results](/files/kanogo-2.png)

After 24 hours, I had a beta running, which worked. Granted, it wasn't great, but it was something that had some value.

I spent the rest of the evening and following morning promoting the beta on mailing lists and on twitter. By the end of the following day I had 30 or so beta signups.

It's already adding value to beta users. Two sites using the beta already on their own products. One beta user has now decied to implement a feature as he's realised his customers consider it a &quot;must have&quot;. There's no substitute for real feedback.

## Learnings

Some of the things I've learned so far:

* *Cloud tools are the business.* It was so easy to register the domain with [dnsimple.com](http://dnsimple.com), start up a [twitter account](http://twitter.com/kanagoapp) for marketing and customer interaction, deploy to [Heroku](http://heroku.com), get initial beta signups with [Campaign Monitor](http://campaignmonitor.com).

* *Modern development tools rock.* I used Rails 3.1 for this app, which worked beautifully, and I love the use of sprockets to help manage the asset pipeline. Running the app on Heroku cedar went without a hitch. I used twitter for authentication, and it only took an hour to set up.

* *There is no &quot;quick and dirty&quot;.* The app is (almost) fully tested: I confess I left a couple of methods only covered by end-to-end tests (which doesn't really count). I definitely proved that the only way to go fast is to go clean: [Jason was right](http://agileage.blogspot.com/2011/07/slow-and-dirty-rant-by-jason-gorman-at.html) that there is no &quot;quick and dirty&quot; only &quot;slow and dirty&quot;. This came back to bite me instantly: the code I didn't use specs for took me the longest to get working.

* *Technology is the easy part.* It didn't take me long to build the site, but the trick is to build a business. After initial interest, the analytics on the site are way down as the next new thing appears on the internet and people move on. To gain traction I need to build the app my beta users actually want. Thankfully, quick feedback is what Kanogo does, so we're eating our own dogfood and asking our users what they think at every turn. This is already directing which features I work on next, which has to be the most efficient way of moving forward, right?

## What's next?

I plan to continue working on this, listening to beta user feedback, refining the features, and accepting new beta signup for the moment. I hope to turn this into a paid product at some point, as I think there's a huge amount of value here to websites if I can get the messaging right.

## Can I get involved?

Sure! It's not too late to join the beta: you can [do so here](http://kanogo.com). I'd love your feedback on the product. It can give you value anywhere you have users of a website, even on a blog as shown above.
</content></entry><entry><title>How to get Spork working NOW on Rails 3, Rspec 2 and Cucumber</title><category term='rails'/><category term='ruby'/><category term='rspec'/><category term='spork'/><category term='cucumber'/><link href='http://chrismdp.com/2010/11/getting-spork-working-now-on-rails-3-rspec-2-and-cucumber'/><updated>2010-11-16T21:41:57+00:00</updated><id>http://chrismdp.com/2010/11/getting-spork-working-now-on-rails-3-rspec-2-and-cucumber</id><content type='html'>I've spent the evening trying to get [Spork](https://github.com/timcharper/spork) to work with Rails 3 and RSpec 2. I've never felt the need for it before, but the Rails 3 start up time is fairly hefty and I'm crying out for the extra seconds more than ever.

It's not that tricky, thankfully, and the following steps should see you running faster specs and features in no time.

## RSpec 2

Follow these instructions to get RSpec 2 working:

*Install Spork into your Gemfile, and update rspec to 2.1:*

{% highlight ruby %}
gem &quot;spork&quot;, :git =&gt; &quot;git://github.com/chrismdp/spork.git&quot;
gem &quot;rspec-rails&quot;, '&gt;= 2.1.0'
{% endhighlight %}

You'll need [my fork of Spork](http://github.com/chrismdp/spork) for a quick patch to the latest release candidate of Spork.

*Add `--drb` on a new line in your .rspec file:*

If you don't have the .rspec file, create it.

*Modify your spec_helper.rb:*

You could follow the installation instructions, but not everything is relevant to Rails 3 and Rspec 2. It's pretty simple anyway: add &quot;require 'spork'&quot; to the top of your spec_helper.rb file, and put everything else inside spec_helper.rb inside a Spork.pre_fork do ... end block:

{% highlight ruby %}
require 'spork'

Spork.prefork do
  ENV[&quot;RAILS_ENV&quot;] ||= 'test'
  require File.expand_path(&quot;../../config/environment&quot;, __FILE__)
  require 'rspec/rails'
  ...
end
{% endhighlight %}

That should be it. To start up the server, run:

{% highlight bash %}
$ bundle exec spork
{% endhighlight %}

...and then try running a spec or two. The following command takes about a second on my machine now, whereas it used to take about ten seconds!

{% highlight bash %}
$ bundle exec rspec spec/controllers/sessions_controller_spec.rb
{% endhighlight %}

## Cucumber

It's important to note that for more than about 10-20 scenarios, Spork is *slower* than running cucumber normally. Therefore only turn it on for a few profiles, such as autotest (but not autotest-all), wip, etc.

*Modify your cucumber.yml file:*

{% highlight yaml %}
wip: --drb -tags @wip:3 --wip features
autotest: --drb --color --format progress --strict
{% endhighlight %}

Leave 'autotest-all' and 'default' alone.

*Modify your features/support/env.rb:*

This is just the same process as with the spec_helper.rb file for RSpec:

{% highlight ruby %}
require 'spork'

Spork.prefork do
  ENV[&quot;RAILS_ENV&quot;] ||= &quot;test&quot;
  require File.expand_path(File.dirname(__FILE__) + '/../../config/environment')
  require 'cucumber/formatter/unicode' # Remove this line if you don't want Cucumber Unicode support
  require 'cucumber/rails/rspec'
  ...
end
{% endhighlight %}

Again, that should be it. Run the follow to try it out:

{% highlight bash %}
$ bundle exec spork cucumber
{% endhighlight %}

Now try running a single feature in rerun or autotest mode. I'm getting 20% speedups for about 10 scenarios.

## Using them together

The RSpec and Cucumber versions of spork use different ports, so there's no problem running them together. Normally I run both in the same terminal window, one as a background process:

{% highlight bash %}
$ bundle exec spork cucumber &amp; bundle exec spork
{% endhighlight %}

Then I run autotest in another window.

## How do I use this?

I'm really liking this setup. It makes rapid TDD possible again, even when dealing with fairly slow tests. 

Of course, we should be doing all we can to get the speed of our tests as high as possible: slow tests are a type of code smell. However, infrastructure load time is unavoidable and cutting it out is full of all kinds of win.

Use this setup with [autotest](https://github.com/grosser/autotest) and [autotest-growl](https://github.com/svoop/autotest-growl) for maximum win. Autotest has come a long way recently: there's a lightweight alternative to ZenTest now, and easy growl support. Cutting out even the 'Oh, I should run my tests now step' totally nails your debug cycle: not sure it gets much tighter than that.

## UPDATE: Even more speed!

[Jo Liss](http://opinionatedprogrammer.com/) got in touch: she's made some performance gains by skipping the &quot;bundle exec&quot; and requiring a few extra files in the prefork block. Read about what she has to say [here](http://opinionatedprogrammer.com/2011/02/profiling-spork-for-faster-start-up-time/).
</content></entry></feed>
