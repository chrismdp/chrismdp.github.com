---
layout: post
title: "My AI Predictions for 2026"
date: 2025-12-31 12:00:00 +0000
categories:
- ai
- predictions
image: /assets/img/ai-predictions-2026.jpg
infographic: /assets/img/ai-predictions-2026-infographic.jpg
---

Everyone says AI progress is slowing down, but I am betting the opposite. I am making some specific predictions for 2026:

- **The adoption gap closes** as people discover current capabilities
- **A major data breach from agent misuse** complicates adoption
- **Google overtakes OpenAI** on model quality
- **Cursor and other AI IDEs start losing market share**
- **20% of code written straight to PR** without intermediate review
- **US developer job growth hits 0%**, not the 1.5% BLS projects
- **The first fully agent-run business emerges**

<!--more-->

## The Adoption Gap Closes

AI progress has felt stagnant to many people, but [threshold crossings become visible](/ai-progress-is-not-slowing-down/) in 2026. Fractional improvements in per-step reliability compound across chained turns, making whole outputs dramatically better than the sum of their parts.

Consider what happens when you chain ten AI steps together. If each step improves from 80% to 85% reliability, that feels like a marginal gain. But when you multiply those probabilities across a chain, the difference between 0.80^10 and 0.85^10 is the difference between 10.7% and 19.7% end-to-end success. Nearly double the reliability from a 5% improvement per step.

People who gave up on AI six months ago will [try it again](/how-to-react-to-a-new-frontier-model/) in 2026 and discover the training wheels can come off. We will see fewer workflows where humans decide every step, and agents allowed to work more autonomously. The gap between perceived and actual capability finally starts to close.

## Security Reckoning

The security problem remains unsolved in 2026, and consequences arrive. Some organisations trust agents too much and get burned, which is the prediction I am most confident about and most worried about.

OWASP and 12-factor fundamentals become more important, not less. The basics of application security, input validation, least privilege access, and proper secrets management are exactly what you need when deploying agents. Organisations that skipped these fundamentals because "it is just an internal tool" will discover that agents with database access and API keys are a liability.

Security-conscious organisations will use AI for "safe" tasks while watching more reckless competitors get burned. Code quality might actually improve as AI handles low-risk tech debt paydown, the kind of mechanical refactoring that humans find tedious but AI handles well.

**I expect a major data breach from agent misuse in 2026**, further complicating an already uneven adoption picture. This will set back AI adoption in enterprise by creating a new round of fear and compliance requirements.

## Google Overtakes OpenAI

This is my most contrarian prediction: Google's structural advantages compound while OpenAI's momentum-dependent model stalls.

Google has massive cash reserves while OpenAI depends on continuous fundraising. They have the DeepMind talent pool, arguably the deepest ML research bench in the world. Imagen is already the best image model. Their two-year reorientation after the ChatGPT shock is now paying off. And they have built-in distribution through Workspace that OpenAI cannot match.

OpenAI has a stagnant year in 2026. The o1 and o3 reasoning models are impressive, but the pace of improvement slows. More concerning for OpenAI, they struggle to raise at their previous valuations. **The company that defined the current AI era [could be in real trouble](/openai-code-red-your-window/) by the end of 2026.**

## Chinese Models Catch Up

Chinese models reach Claude 4.5 and ChatGPT 5.2 levels, wherever we are by end of 2025, enabling open and cheap API access. But mainstream users will not notice. They stay on the main UIs like ChatGPT and Claude.ai, which are [terraforming how we think](/chatgpt-is-terraforming-earth/). No significant UI improvement happens in 2026 to change this pattern.

Values sovereignty matters here. 90% of people sleepwalk into this problem as AI permeates society further without awareness of embedded cultural assumptions. When your AI assistant is trained on different values, optimised for different outcomes, and [subtly shapes your thinking](/the-slow-and-dangerous-loss-of-self/) in different directions, does it matter? Most people will not even ask the question.

Developers and technical leaders will increasingly use Chinese models for cost-sensitive applications. The quality gap closes while [the price gap remains enormous](/doing-real-work-with-ai-just-became-150x-cheaper/). This creates a bifurcated market: premium Western models for customer-facing applications, cheap Chinese models for internal tooling.

## Agent Orchestration Emerges

PRD-driven parallel development becomes more viable as we get better at structuring agents. The top 1% of developers get exponentially faster while most people improve incrementally. A new tool for orchestrating independent coding agents gains meaningful adoption by December 2026. We have already seen [how different the current options are](/three-coding-agents-head-to-head/), and this fragmentation continues before consolidation.

**Cursor and other AI-native IDEs start to lose market share** as the interaction model shifts. Instead of AI-assisted coding in an IDE, we move toward AI-driven coding with human oversight. The IDE becomes less central when you are reviewing PRs rather than writing code. [Steve Yegge argues](https://www.youtube.com/watch?v=zuJyJP517Uw){:target="_blank"} that Claude Code is not the answer either, and he is right: the [tools are not quite ready yet](/independent-coding-agents-tools-arent-ready/), but 2026 is when they mature.

**By the end of 2026, 20% of all production code will be written straight to PR** without intermediate human review. This sounds alarming, but think about what it means. If you have good tests, good CI, and good code review processes, why does it matter whether a human or an AI wrote the first draft? The review is what matters.

AI code-writing networks working in tandem start to emerge. Multiple specialised agents collaborating: one for architecture, one for implementation, one for testing, one for documentation. [Swarm approaches have failed me so far](/ai-swarms-failed-toyota-vs-ford-development/), and 2026 is when we figure out how to orchestrate these effectively.

## Developer Job Market Squeeze

Talent matters more than ever, but mediocrity gets squeezed at all levels.

The winners are talented juniors who use AI to operate like seniors, quickly closing the experience gap. Seniors remain protected by organisational knowledge, the understanding of why systems are built the way they are, and the context that AI cannot access. Anyone who understands maths, concepts, and theory rather than just syntax will thrive.

The losers are mediocre mid-level developers, squeezed by capable juniors with AI on one side and seniors with irreplaceable context on the other. AI is [consistently mediocre](/ai-is-consistently-mediocre/), which means it competes directly with mediocre humans. Mediocre juniors who rely on AI as a crutch rather than a learning tool. Applied-only coders whose value was in translating requirements to syntax, exactly what AI does well.

The skills that matter shift decisively. Reading code fast becomes more valuable than writing it. Strategic thinking about codebases matters more than implementation speed. Understanding maths and concepts trumps knowing API signatures. Syntax and boilerplate coding become nearly worthless.

**US software developer jobs will show 0% growth in 2026**, not the 1.5% annual growth the BLS projects based on historical trends. The productivity gains from AI finally start showing up in headcount decisions. Companies discover they can ship the same amount with fewer developers, or ship more with the same number. Either way, the hiring boom ends.

## First Fully Agent-Run Business

**At least one business emerges in 2026 that is entirely run by agents**, with humans operating only as board-level non-executives. It will not be proven successful yet, with modest revenue, shaky operations, and plenty for sceptics to criticise.

But it will start growing, become famous or perhaps infamous, and hold its own against competitors with traditional human workforces. The proof of concept matters more than the proof of profit in 2026.

This is less a prediction about AI capability and more a prediction about human ambition. Someone will try this, probably several people, and at least one of them will make it work well enough to survive the year and attract attention.

## What Will Not Happen in 2026

Some things are building but will not reach tipping points in 2026.

[Energy abundance](/ai-power-usage-reality/) and the solar thesis remain on a longer horizon. The infrastructure investments are happening, but the transformation takes longer than optimists expect.

[Model collapse](/ai-slop-is-real-model-collapse-isnt/) stays masked by raw capability gains. We will not see clear evidence of AI training on AI output degrading quality, even though it is probably happening. The capability improvements are large enough to hide the effect.

Evaluation infrastructure does not become standard. Most teams still [evaluate on vibes](/how-to-build-a-robust-llm-application/) rather than proper metrics. The tooling exists, but adoption lags. We are still in the "spreadsheet era" of AI evaluation, tracking things manually rather than systematically.

Production agent tooling remains in the diffusion phase. The patterns are emerging, but no dominant framework wins. Everyone is still experimenting with different approaches to agent reliability, memory, and orchestration.

---


I will revisit these predictions in December 2026. My purpose in making predictions is not to be right, but to be specific enough that I can learn from being wrong. These are bets, not wishes (some of them hopefully will not come true), and I look forward to seeing how they age.
